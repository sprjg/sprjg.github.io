[{"title":"Fourier Analysis","href":"/posts/20200524030032-pubnotes_ddse_fourier","content":"This is for my notes for Prof Steve Bruntons video lectures of Fourier AnalysishttpswwwyoutubecomwatchvjNC0jxb0OxElistPLMrJAkhIeNNT5FXh3Oy0Y4LTj0Oxo8GqsC Real domain to complex domain realdomaintocomplexdomain The first confusion for me was the shift in the sum limits from real domain to complex domain The real fourier series is expressed as sumk0infty fraca02 ak cos kx bk sin kx The intuition here is that harmonics can express any function The constant term accounts for all the different initial phase shifts of each of these harmonics The mathematical property of these trigonometric terms sinkx coskx is that they are orthogonal Their inner product intpipi fx gx dx 0 To verify The conjugation on the second term is irrelevant for real valued functions fg can be any sinkx coskx term This lecturehttpsocwmiteduresourcesres18008calculusrevisitedcomplexvariablesdifferentialequationsandlinearalgebrafall2011studymaterialsMITRES5F185F0085FpartIII5Fsol08pdf goes into detail about each combination The intuition is that intpipi sin ax cos bx is an odd function so integrating over this particular limit will be zero But when ab int sin2 ax int cos2 ax are reduced using trigonometric identities to a constant plus a periodic function and the constant part evaluates to pi All of the above is for the real domain In complex domain the fourier series becomes sumkinftykinfty ck eikx where ck in mathbbC The sum starts from negative infinity now unlike the real domain Oscillations in real domain rotations in complex domain oscillationsinrealdomainrotationsincomplexdomain In real domain the eigenfunctions are either sin kx or cos kx This means that sinkx and sinkx are not orthogonal This can be verified by the inner product which evaluates to pi So negative k in the real fourier series redundant as the eigenfunctions of k 0 already include the k 0 part too Another way to think about this is phase shift k 0 terms All the phase shift terms are already accounted for by a0 But in complex domain the eigenfunctions represent a rotation at different frequencies and starting points rather than oscillation k 0 represents an anticlockwise rotation This videohttpswwwyoutubecomwatchvspUNpyF58BY helps In this case k 0 functions The computation of the inner product confirms the same of course TODO The complex conjugation of the second term in the inner product is equivalent to a direction change of rotation I know that the integration of a complex function is related to contour integration but I dont see how the inner product in the complex domain represents closeness of two rotations Yet to read thishttpwww1spmsntuedusgydchongteaching085Fcontour5Fintegrationpdf Backlinks backlinks DDSE compressed sensing with DFT Now we compute the FFT and confirm with the result previously Scaling Learning Algorithms towards AI It seems theres also a relation to the Fourier Analysis in terms of layers The authors suggest that functions with high frequencies in their fourier spectrum cannot be represented with two layer architectures Linial et al 1993 is the source of this finding This is extremely interesting but also a rabbit hole Is it worth going into this now Ill have to come to back to this The summary from this section is that shallow architectures are poor at describing more complex functions"},{"title":"Compressed Sensing with the Fourier Ensemble","href":"/posts/20200611111551-compressed_sensing_with_the_fourier_ensemble","content":"Im reading the topic of compressed sensing and I find it extremely dense with a lot of historical perspective This videohttpswwwyoutubecomwatchvaHCyHbRIz44 is a great start to the topic Reproducing the example reproducingtheexample The example at 1405 in the videohttpsyoutubeaHCyHbRIz44t846 uses a DCT The cosamp function used below is from this repositoryhttpsgithubcomavirmauxCoSaMP The code here is slightly modified from the jupyter notebook available herehttpdatabookuwcom The sample audio signal thesampleaudiosignal python stx 0 end 1 n 4096 Fs int nend stx from math import pi from numpyfft import rfft rfftfreq t nparangestx end 1Fs x npcos2 97 pi t npcos2 777 pi t f rfftx psd ffn freqs rfftfreqn Fs pltclose pltfigure pltplotfreqs f r fint npfloorabsf filename audio reconstruction compressed sensing pltsuptitlefilename pltsavefigffilenamepng pltclose xfft npfftirfftf pltsaveinverse fourier check compressed sensing lambda pltplott xfft r pltsavezoom in inverse fourier check compressed sensing lambda pltplott xfft r pltxlim025033 text homeplocallibpython36sitepackagesnumpycoreasarraypy85 ComplexWarning Casting complex values to real discards the imaginary part return arraya dtype copyFalse orderorder The code thecode python p 128 from scipyfftpack import dct idct In npdiagnponesn Psi dctIn axis0 Psi splinalgdftnreal pltsavePsi matrix heatmap cosamp lambda pltimshowPsi cmaphot interpolationnearest idxs nprandomrandp nastypeint y xidxs pltsavemarked measurements of original signal for compressed sensing cosamp lambda pltplott x tidxs y rx pltxlim025 033 Theta Psiidxs pltsaveTheta matrix heatmap cosamp lambda pltimshowTheta cmaphot interpolationnearest xest cosampTheta y 3 Score estimation printfScore estimation nplinalgnormx xest nplinalgnormx pltsaveresult heatmap dct cosamp lambda pltplott xest xdct idctxest pltsavecompare original signal with recovered signal from cvxopt dct cosamp lambda pltplott xdct r t x k pltxlim025 033 text main35 FutureWarning rcond parameter will change to the default of machine precision times maxM N where M and N are the input matrix dimensions To use the future default and silence this warning we advise to pass rcondNone to keep using the old explicitly pass rcond1 Score estimation 10002591450121945 While I can reproduce the result from the video I find the selection of basis confusing If the random drawing of measurements is from the DCT matrix then is it equivalent to a measurement in the frequency domain But the code is clearly measuring from the time domain samples For example could I use the Fourier basis and achieve the same results Simply changing from DCT to DFT does not work I tried Interestingly I recover back a signal which has the correct frequency spikes but very low energy So the resulting estimation is scaled down by 3 orders but the frequency content matches Back to basics backtobasics From the original workhttpsarxivorgabsmath0410542 Sec 11 the authors define FOmega f hatfk k in Omega The Omega is a set of frequencies sampled uniformly in random The reconstruction g is obtained by min g l1 min Sigmat in mathbbZn gt subject to hatgk hatfk forall k in Omega In the above signal the sparsity is expected in the frequency domain ie we minimize the l1 norm in the frequency domain The equivalent problem formulation theequivalentproblemformulation min hatg l1 subject to gt ft forall t in T and the result is highly probable to be bhatf The Theta matrix thethetamatrix FOmega now is the random drawing of rows from the ifft Using IDFT as Psi usingidftaspsi Psi can also be created from applying the inverse fft to a identity matrix since its an unitary operation The idea is to randomly sample rows of the Psi matrix and create a theta python random indices of the DFT between 0 and n 4096 p 128 idxs nprandomrandp nastypeint In npdiagnponesn idft npfftifftInreal python Theta idftidxs pltsaveTheta heatmap for fft basis lambda pltimshowTheta cmaphot interpolationnearest The measurement y is a sparse signal taken at random indices of the actual x beginsrc python session compressedsensing measurement is random sampling at the above indices y xidxs Using file20200608115955convexoptimizationorgconvex optimization We can use convex optimization to solve for the sparsest s sres optminimizelambda e nplinalgnorme 1 x0 npzerosn constraints type eq fun lambda e Theta e y sres cosampTheta y 10 pltsaveresulting sparse in FFT basis lambda pltplotsres pltimshownpmatrixsres cmaphot interpolationnearest xfft npfftifftsres pltsavecompare original signal with recovered signal from cosamp dft lambda pltplott xfft r t x k pltxlim025033 text main35 FutureWarning rcond parameter will change to the default of machine precision times maxM N where M and N are the input matrix dimensions To use the future default and silence this warning we advise to pass rcondNone to keep using the old explicitly pass rcond1 homeplocallibpython36sitepackagesnumpycoreasarraypy85 ComplexWarning Casting complex values to real discards the imaginary part return arraya dtype copyFalse orderorder Results results So drawing from the inverse fft matrix does work The recovered basis also shows the spikes at the right frequencies Unresolved confusions unresolvedconfusions How is it possible to draw from the DCT matrix while measuring the time domain samples Using DCT as Phi does lead to correct recovery of the original frequencies so theres no flaw in the method Units of the recovered DCT basis yaxis"},{"title":"lassoPlot in python","href":"/posts/20200624220142-lassoplot_in_python","content":"A comparison of python vs matlab lassoPlot in matlab lassoplotinmatlab In matlab lasso regressions MSE is easily visualized using the command lassoPlot The following is from Prof Steve Bruntons uwdatabook pagehttpuwdatabookcom In the zip file the exact file is CH03SEC052LASSOm The most relevant parts to create the plot are picked out here nil A randn10010 Matrix of possible predictors x 0 0 1 0 0 0 1 0 0 0 Two nonzero predictors b Ax 2randn1001 Observations with noise xL2 pinvAb XL1 FitInfo lassoAbCV10 lassoPlotXL1FitInfoPlotTypeCV save See output below Equivalent command in python equivalentcommandinpython In python the function calls are completely different In fact there no direct way to mention 10fold cross validation like in lassoAbCV10 This answerhttpsstackoverflowcomquestions12746479whyarelassoinsklearnpythonandmatlabstatisticalpackagedifferent2045740220457402 shows how to create the KFold cross validation model first and then use it in LassoCV In the previous matlab code sample I saved the variables so as to recreate the exact plot in python python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt from scipyio import loadmat import sklearnlinearmodel no effect on algorithm plotting helper function def pltsavetitle funcarr Helper function for laying out plots quickly title plot title titlepng is the file name given to savefig funcarr array of functions one each executed for a column of subplots each function can include plotting axes changes xlim ylim calls Basically anything specific to that subplot can be done in that function pltclose fig axs pltsubplotslenfuncarr 1 squeezeFalse constrainedlayoutTrue for ax func in zipaxsT0 funcarr funcax pltsuptitletitle pltsavefigftitlepng pltclose alldata loadmatmatlabmat this represents the measurement A alldataA x alldatax b alldatab with cross validation kf sklearnmodelselectionKFoldnsplits10 shuffleTrue printsklearnmodelselectionKFolddoc cv sklearnlinearmodelLassoCVcvkf normalizeTruefitAb msepath contains the errors for each fold alpha pair we want to plot the variation of errors in each alpha across the folds ie alpha on xaxis the y spread shows the error spread eps 001 the smaller it is the longer is the path alphaslasso coefslasso sklearnlinearmodellassopathA b epseps mean lambda e sumelene minmses mincvmsepathi for i in range0 lencvalphas maxmses maxcvmsepathi for i in range0 lencvalphas meanmses meancvmsepathi for i in range0 lencvalphas devmses npstdcvmsepathi for i in range0 lencvalphas idxalphaofminmse npwherenpisclosecvalphas cvalpha00 minmseoveralphas meancvmsepathidxalphaofminmse standard deviation of the error at the best fit MSE dev devmsesidxalphaofminmse printnprounddevmses2 printfmeanmses nproundmeanmses 2 printfdev nprounddev2 find closest alpha for which mean mse is close to dev idxssparsealphacandidates npwhere npisclose meanmses nponeslikemeanmses minmseoveralphas dev atol01 printmeanmses minmseoveralphas dev printfidxssparsealphacandidates idxssparsealphacandidates idxsparsestalpha idxssparsealphacandidates00 printidxsparsestalpha sparsestalpha cvalphasidxsparsestalpha printfcvcoefcvcoef printfsparsestalpha sparsestalpha printfoverfitalpha cvalphasidxalphaofminmse without cross validation at sparsest alpha clf sklearnlinearmodelLassoalphasparsestalpha clffitA b printfclfcoef clfcoef identify the active clf coefs idxactive npwherenplogicalnotnpiscloseclfcoef npzeroslikeclfcoef printnpisclosedoc printfidxactive idxactive Aactive npzeroslikeA Aactive idxactive A idxactive printnpshapeAactive printnpshapeb xestimateactiveonly nplinalgpinvAactive b printnproundx xestimateactiveonly 2 pltsavepython lasso demo LassoCV show error spread lambda ax axplotcvalphas m d2 for md in zipmeanmses devmses o axplotcvalphas m d2 for md in zipmeanmses devmses o axplotcvalphas meanmses o axplotcvalpha minmseoveralphas x axplotcvalphasidxsparsestalpha meanmsesidxsparsestalpha kx axinvertxaxis axsetxscalelog axplota a m d2 m d2 y for a m d in zipcvalphas meanmses devmses pltsavepython lasso demo lambda ax axplotcvcoefo label10fold cv axplotx x labeltrue values x axplotclfcoef o labelno cv alpha at sparsest axplotl2res o labell2 axplotxestimateactiveonly o labell2 regression correction axplotnplinalgpinvA noise s labelinvA noise axlegend text homeplocallibpython36sitepackagessklearnutilsvalidationpy73 DataConversionWarning A columnvector y was passed when a 1d array was expected Please change the shape of y to nsamples for example using ravel return fkwargs 251 25 245 24 236 233 231 229 228 226 225 224 224 224 224 224 224 225 225 226 227 227 228 228 229 23 23 231 232 233 234 235 236 237 238 238 239 24 24 241 242 242 243 244 245 245 246 247 248 248 249 249 25 25 251 251 251 252 252 252 253 253 253 254 254 254 254 254 255 255 255 255 255 255 256 256 256 256 256 256 256 256 256 256 256 257 257 257 257 257 257 257 257 257 257 257 257 257 257 257 meanmses 554 55 543 536 53 526 523 52 518 516 514 51 508 506 504 503 502 501 501 502 504 506 508 511 514 517 52 522 525 527 529 532 534 536 539 541 543 544 546 548 549 551 552 554 556 557 559 56 561 562 563 565 565 566 567 568 569 57 57 571 572 572 573 573 574 574 575 575 575 576 576 576 576 577 577 577 577 578 578 578 578 578 578 578 579 579 579 579 579 579 579 579 579 579 579 579 58 58 58 58 dev 225 idxssparsealphacandidates array dtypeint64 Traceback most recent call last File line 1 in File tmpbabelNg5Fd9pythontkuHOj line 69 in idxsparsestalpha idxssparsealphacandidates00 IndexError index 0 is out of bounds for axis 0 with size 0 Important differences importantdifferences Not the same cost function notthesamecostfunction The python function call LassoCV and matlab function call lasso do not solve the same optimization problem LassoCV solves a normalized version of the problem by diving the cost function by the number of samples This changes the resulting penalty values being used in the two versions In matlab using lambda in ab is equivalent to using lambda in na nb in python See matlab lasso documentationhttpswwwmathworkscomhelpstatslassohtml end of the page Compare to scipy LassoCV documentationhttpsscikitlearnorgstablemodulesgeneratedsklearnlinear5FmodelLassoCVhtml This is visible from the xaxis of the plots and you see a they are off by a factor of 100"},{"title":"Algo walkthrough: swapping elements to find max score","href":"/posts/20201122105230-algo_walkthrough_swapping_elements_to_find_max_score","content":" Problem statement problemstatement Given an array of size n the beauty of this array is defined as sum of absindexvalue for each element Swapping two elements of this array what is the maximum beauty possible Initial setup initialsetup The initial setup consists of an unsorted array Each element has a score defined as the absolute value of the difference the elements value and its index The absolute value theabsolutevalue The metric for each element gives us a sense of displacement For example if an element at index 4 is of value 10 the score for this is 6 If the element was at index 5 and of value 11 the score is still 6 This suggests that we are to think in terms of displacement Displacement from what displacementfromwhat Here we can think of the two parts in the difference We have an index and we have a value We could think of the elements score as its displacement Currently the element is at index i The displacement is i e So originally it was at e A new of approaching this anewofapproachingthis Imagine we have a row of bins We are randomly filling its slots with the index of the slot So we can choose to fill slot 2 with the value 2 In fact we are allowed to fill the same slots multiple times forget for now how this will be represented in an array as arrays only allow one value in a slot The bins are now filled Suddenly each of the elements start moving They can move to the left or to the right They can move at different speeds After a while they all stop Again assume here that we now have a row of elements none overlapping by some miracle So initially we may have multiple elements in the same bin but now that they have randomly moved around they now occupy unique slots Now we can represent them in an array The score thescore Now we can think of the score for each element as the amount it has moved from their original bins The problem theproblem The question for us is to swap the position of two elements in this array so that we increase the total score of the the whole array This might have multiple possible solutions so instead we are looking for the pair of elements when swapped will increase the total score the most Details of the question detailsofthequestion Increasing the score of one element increasingthescoreofoneelement Is there a way to increase the score of just one element The score for an element is i e The elements value e is fixed The i can be changed but not in isolation Because we are using an array we are assuming that the indices are running contiguously So every index must be occupied by some valid element and there cannot be any gaps So we definitely need another element which can take this elements place Swap swap This means swapping is the minimal atomic way of changing the score of the array During a swap we change the indices of two elements in the array so only the scores of these two elements change By changing these we are changing the total score Useful swaps usefulswaps Can we change any two elements to achieve a change in score In other words is there a way to reduce our search space Model 1 model1 Say we represent an element as a tuple with its index and the value i e Now lets take two elements i1 e1 i2 e2 Their scores are i1 e1 i2 e2 Now lets define what swapping means When we swap we actually swap the indices of the elements leading to i2 e1 i1 e2 The scores now become i2 e1 i1 e2 What is the increase in the score i2 e1 i1 e2 i1 e1 i2 e2 At this point we have to take different cases to analyze i1 e1 i2 e2 i1e1i2e2 In this case the score is simply i1 e1 i2 e2 before swapping and i2 e1 i1 e2 after swapping Here again there can be multiple cases i2 i1 e1 i1 e2 i2i1e1i1e2 In this case the change in score becomes zero remove the modulus and check for yourself i2 i1 e1 e2 i1 i2i1e1e2i1 In this case the change in score is i2 e1 e2 i1 i1 e1 i2 e2 e2 i1 i1 e2 2e2 i1 Model 2 model2 As we see it definitely helps to analyze elements as a tuple We were able to rule out some parts of the search space But one issue is the number of different cases we could have It might help to think of the elements as points in space Lets try thinking of indices running along the Yaxis and plot the points as lengths running along Xaxis An example can help Say the array 4 5 0 1 python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2darray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray pltplotlistrange0 lenarray listrange0 lenarray b for idx e in enumeratearray pltplotrangemine idx maxe idx 1 idx 1 absidx e k axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltsavefigfilename pltclose array 4501 plotarrayas2darray swapssamplesmallarraypng length 20 array randomchoicelistrange0 length for i in range0 length plotarrayas2darray swapssamplearrayplotpng The blue line is the xy line ie indexvalue line Some immediate observations Each elements score is the black line Imagine moving the element to a different index while retaining the Value and try to see how the black lines length changes When elements are swapped their Value remains the same but their score changes because the distance to the blue line changes Fallacy Elements on left of the blue line seemingly have no benefit in moving down as this reduces the score But this is not true It seems this way because of the example See next example Fallacy Elements on right of the blue line seemingly have no benefit in moving up as this reduces the score But this is not true It seems this way because of the example See next example Two elements whose score lines ie the black lines are overlapping have no benefit swapping as this reduces their scores Its time to expand our example to check our observations A larger array example to see more observations alargerarrayexampletoseemoreobservations We have seen certain observations in the previous visualization Lets see how many of them still remain valid with a larger example oxhugoswapssamplearrayplotpng Overlapping score lines before and after swap overlappingscorelinesbeforeandafterswap This definitely seems to hold true Notice the elements at index 6 and 8 They are on the same side of the blue line They have an overlapping score line On swapping whatever is gained by 6 is lost by 8 leading to no change in the total score Notice the elements at index 18 and 17 They are on opposite sides of the blue line They have a small overlap On swapping this overlapping part is lost by both sides leading to decrease in scores for both ie decrease in total score Notice the elements at index 5 and 7 They are on the same right side of the blue line They have an overlapping score line On swapping whatever is gained by 5 and 7 leading to no change in the total score Reverse of above cases reverseofabovecases For each of the cases above notice that the reverse causes the opposite effect for a zero change in score the opposite is also zero change in score The second case is interesting The decrease in score on removal of the overlap implies that causing an overlap increases score Well come back to this Fallacy No benefit in crossing the blue line fallacynobenefitincrossingtheblueline This seems to be a fallacy in our previous observation Notice how element at 15 could in fact move down all the way crossing the blue line and achieve a better score assuming we can have a partner who wants to move up Summary summary On swapping one element moves up one element moves down We want an increase in score and we have seen thatorg8bcaf3e causing an overlap increases score We also see that an element can cross the blue line to increase its own score Together with the above two observations we can say that if we pick any two nonoverlapping score lines and swap them there is a definite increase in total score Take 4 and 14 On swapping we get an increase in score which is equal to 2 x gap between the lines Take 6 and 9 On swapping we get an increase in score which is equal to 2 x gap between the lines Strategy strategy We have reduced the search space to only nonoverlapping elements This is still a large search space of say s and our current understanding is to iterate over s2 options and find the maximum increase Is there a way to reduce our search space further Model 3 model3 From our observations in Model 2 we know that any two elements whose score lines dont overlap can be swapped to achieve an increase in total score With this in mind we can approach this problem with a fresh perspective Lets start by looking at each element as simply an interval Think of a horizontal line with a start and an end Out of many such horizontal lines we are looking for two that maximize spacing between them Visualizing intervals visualizingintervals Lets try to see this in action python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2dintervalsarray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray for idx e in enumeratearray pltplotrangemine idx maxe idx 1 idx 1 absidx e k linewidth10 if e idx pltscattere e markers colork s100 axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltgrid pltsavefigfilename pltclose array 4501 plotarrayas2dintervalsarray swapsmodel3samplesmallarraypng length 20 array randomchoicelistrange0 length for i in range0 length plotarrayas2dintervalsarray swapsmodel3samplearrayplotpng text Python 385 default Sep 7 2020 110458 GCC 750 on linux Type help copyright credits or license for more information pythonel native completion setup loaded oxhugoswapsmodel3samplearrayplotpng Note that this is just Model 2 without the blue line Our next step is to form a strategy to eliminate our search space fast Transitivity transitivity One way to eliminate parts of search space is by using a transitivity property We need a way to show that two elements e1 e2 cannot form the optimal swap then we can eliminate a set of elements related to e1 and e2 For example 0 and 5 are already overlapped So swapping them only leads to nonoverlap and a reduction in score Note that 1 2 3 4 are all elements that have a starting point thats before 5 but after 0 So if 5 overlaps with 0 then all these elements 1234 also will overlap with 0 We can eliminate them for pairing with 0 without checking Another example Note that 0 and 18 overlap but 0 and 17 do not So this is not about the indices but rather the starting point Here even though 18 has a higher index than 17 the starting point 17 is after 18 So we cannot eliminate 17 based on test of 0 and 18 But we can eliminate pairing 0 with 7 6 5 4 3 2 as they all have starting points before 18s starting point Achieving transitivity achievingtransitivity Transitivity is useful but we need an apt data representation to exploit this In our current representation we just discussed how we are unable to eliminate 017 by testing for 018 This is because the starting points are not related to the indices This gives us a clue Lets try sorting our intervals based on the starting point Remember that we are not sorting the original array that would defeat the purpose as we are only supposed to make one swap and sorting it is a lot more swaps We are simply changing our visualization of the original array to look for any patterns python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2dintervalsarray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray pairs for idx e in enumeratearray pairsappendmine idx maxe idx sortedstart sortedpairs keylambda e e0 for idx e in enumeratesortedstart pltplotrangee0 e1 1 idx 1 abse1 e0 k linewidth10 if e0 e1 pltscatteridx idx markers colork s100 axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltgrid pltsavefigfilename pltclose plotarrayas2dintervalsarray swapsmodel3sortedsamplearrayplotpng We now have a way to tell which two elements are most apt for pairing It seems the elements picked one each from the two extremes would give us a good spacing But is it the best May not be We can see element at 1 not element at 0would be a better choice to pair with 19 We can see why because 0 ends before 1 does and so has more spacing with 19 Note that now have transitivity almost We can start with the extremes and say for sure that these may be our best bet Close to the end closetotheend Our observations from previous section show that a good starting point is crucial We say good to mean smallest In our representation we are pairing a small starting point element with a large starting point element But our fallacy was coupling starting and ending points Ending point value is completely independent from starting point value for each element So what is a desirable ending point value Wed like it to be as close to the starting point as possible This makes the metric for ending point dependent on the elements starting point In fact this is simply the score for each element So for the first element of the swap we are looking for an element that starts early and last a small score For the other element of the swap we only care that it starts as far as possible and not about the score because the gap doesnt depend on where the second element ends This gives us an idea What if we also sort based on the end point python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2dintervalsarray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray pairs for idx e in enumeratearray pairsappendmine idx maxe idx sortedstart sortedpairs keylambda e e1 for idx e in enumeratesortedstart pltplotrangee0 e1 1 idx 1 abse1 e0 k linewidth10 if e0 e1 pltscatteridx idx markers colork s100 axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd tickerMultipleLocatorMAXTICKS 2000 axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltgrid pltsavefigfilename pltclose plotarrayas2dintervalsarray swapsmodel3sortedendsamplearrayplotpng For comparison lets compare the two representations sort based on start point sort based on end point We are looking for two elements here one that appears relatively close to the bottom in both these plots one that appears relatively higher to the top in both these plots For each element we could sum up their index in both the arrays We can then pick the element with the smallest sum and the element with the largest sum This would give us the correct answer But is there a faster way to find these elements without having to sort the array twice Faster way to find these elements fasterwaytofindtheseelements Say we allow for one sorting Lets sort by start point oxhugoswapsmodel3sortedsamplearrayplotpng Lets try to find a suitable element to be the left side of the swap Well refer to the elements as a tuple of their start point end point Our first option is 0 15 So this element ends at 15 Our best gap at this point starts at 15 We can rule out any element that starts after 15 because they will only be worse ie the gap would start later and not useful So we have already removed part of our search space All elements with start point greater than 15 can be ruled out Lets move on to the next option in our search 12 This gives a gap that starts at 2 which is obviously better our earlier option of 15 And now we can rule out any element that has a start point greater than 2 We have now removed a huge percentage of our search space now Our next option is 13 which means a gap that starts at 3 But we already have a better option that starts at 2 Our options after this all start at 2 which we have already ruled out We are done But this is just half the battle We have only found the left side of the swap We still need the right side of the swap For the right side element our criteria is finding the largest start point ie the gaps end But this is straightforward We can simply take the last element of the current sorted array We dont care about where this element ends because the gap size doesnt change Edge cases edgecases Okay so we have a full solution to find the left and the right side of the swap What happens if we find that these two elements are overlapping This means there are no elements that can be swapped to achieve a better score Our criteria was maximizing the gap when we found these elements But the best gap we can achieve is not positive Tests tests Well write our strategy as code and check if it works against a random array Just to be sure well also compute the n2 solution ie brute force by checking each element with every other element python array randomchoicelistrange0 1000 for i in range0 1000 pairarray mine idx maxe idx for idx e in enumeratearray sortedstartarray sortedpairarray keylambda e e0 beststart sortedstartarray01 bestleft sortedstartarray for e in sortedstartarray1 if e0 beststart if e1 maximprovement maximprovement improvement maxchoice i a j b printPolynomial solution print Maximum beauty achieved is d score maximprovement text Single pass solution Maximum beauty achieved is 330735 Polynomial solution Maximum beauty achieved is 330735 An even faster way anevenfasterway Our strategy currently sort by start point the element with highest start point is the right side of swap among the elements with lower not lowest start points find the one with the lowest end point Notice that the second step is direct confirmation that right side of the swap has to be the element with the highest start point But how come we dont have a simple way to describe the left side of our swap Lets try that In step 3 we are finding an element with the lowest end point But this is among elements that already have low start points But wait if an element has the lowest end point then it doesnt mean it has the lowest start point imagine 0 10 5 6 and compare them So we have stumbled on a new finding For the left side of the swap we only care about the end point This makes sense because we care about the gaps start which is defined by the elements end point regardless of where it starts In our current strategy sorting helps in two ways 1 finding the right side of swap directly 2 while looking for the smallest end point we are able to cut down search space because its a sorted array again by exploiting transitivity A step back astepback Each element has two quantities a start point and an end point Our goal is to find two elements One with the highest start point One with the lowest end point and also a low start point Notice that we no longer care about both quantities in each element We either care about highest start point or about the lowest end point This is crucial Imagine we had an array of dictionaries Each dictionary has two quantities alpha and beta We are asked to find two elements one with lowest alpha and one with highest beta In this case we could simply iterate over the array improving our search as each new element is encountered We are exactly in that scenario We can now avoid sorting and simply iterate over the array This is because we updated our observation for the left side of the swap We are no longer looking for an element that has both good start and good end We are only looking good end For the right side of the swap we were clear that we need to find the highest start point regardless of its end point That should have given us a hint to explore the left side of the swap we need the lowest end point regardless of its start point Tests tests Lets remove the sorting Instead we do a simple iteration where we update our best start point and our best end point The end of the iteration signals the end of our search no transitivity as this is an unsorted array python array randomchoicelistrange0 1000 for i in range0 1000 pairarray mine idx maxe idx for idx e in enumeratearray beststart 1 bestleft pairarray0 bestend 10000 bestright pairarray1 for e in pairarray if e1 beststart beststart e0 bestright e score sumabse idx for idx e in enumeratearray score 2 beststart bestend printSingle pass solution print Maximum beauty achieved is d score CHECK 1 array51 array984 array984 array51 score sumabse idx for idx e in enumeratearray printscore CHECK 2 n2 test score sumabse idx for idx e in enumeratearray maximprovement 0 for i a in enumeratearray for j b in enumeratearray improvement absja absib absia absjb if improvement maximprovement maximprovement improvement maxchoice i a j b printPolynomial solution print Maximum beauty achieved is d score maximprovement text Single pass solution Maximum beauty achieved is 320292 Polynomial solution Maximum beauty achieved is 320292 "},{"title":"Algo walkthrough: longest sequence in unsorted array","href":"/posts/20201123210516-algo_walkthrough_longest_sequence_in_unsorted_array","content":" The setuphttpsleetcodecomproblemslongestconsecutivesequence thesetup We have an unsorted array It may or may not have a sequence of numbers We have to find the length of the longest sequence Hints hints They hint at a On solution This can be useful at a later point They also say the length of the array can be between 1 10000 This means we can a small array of size 10 and manually try out solutions This would probably scale fine Each element can be huge 109 109 Note that the value is higher than the length of the array This will be useful Variations to the setup variationstothesetup It can be useful to think of a series of events leading to this question I think of this as starting from the solution Say we have a sorted sequence of numbers 1 2 3 4 5 6 7 8 9 10 11 Lets add gaps between elements randomly 1 2 3 4 5 6 7 8 9 10 11 Now lets fill random elements to these gaps 1 76 87 2 8 3 21 67 15 4 9 5 50 43 99 6 14 7 29 78 8 41 9 70 10 54 11 Note that this is still not the original problem Here our sequence is still sorted within the noise But just to understand the wrong approaches lets try and solve this Sorted sequence within noise sortedsequencewithinnoise We know that the correct sequence will appear in its sorted order The question hints at On for its setup and what we have is arguably an easier problem So lets aim to solve this in On When we say On for an array we intuitively think of a single pass Lets explore this Some thoughts We get a single pass We see an element and we get to operate on it Because we are thinking single pass we also must have an answer at any point of time Say we have a 100 elements in the array We have processed 20 of them But wait if our array was only of size 20 then we should have the final answer now This means every time we process an element we need a new answer or stick to our previous answer but we need some answer at that point Extra space Based on the previous point we should be thinking of storing our findings and our best guess at any time The single pass thesinglepass Now lets go over the array one element at a time and see what we can uncover First element 1 we have no way to tell whether or not this is part of the sequence But we definitely know that this can either be the start of the sequence or not part of it lets store this in a dictionary called potentialstart with the key as 1 But what can we store in the value part Lets store the length of the potential sequence 1 1 Second element 76 we have more information than before We know for sure this is not part of the potential sequence starting at 1 obviously Because we expect the sequence to appear in order this previous point may push us towards searching for the next number of the existing sequence ie 2 but this can be wrong Because we have no way to know whether the sequence starts at 1 what if the sequence actually starts at 76 No reason why it shouldnt So this is also potentially a start of the sequence Lets add this to our dictionary 1 1 76 1 Third element 87 same reasoning as before This could be the start of a new sequence 1 1 76 1 87 1 Fourth element 2 now we have some hope We can do constant time look up on dictionaries So checking whether there is an element with key 1 one less than our current element is O1 We do have an element with key 1 lets add 2 to the dictionary but make its value 1 1 2 Because we already have an ancestor to it 1 1 76 1 2 2 what if the array stops here What if there are no more elements Do we have an answer No We dont have the length of the longest sequence ie the largest number in the dictionary So lets maintain that also We can update this maximum value as and when new elements are added Currently this maximum counter value is 2 I hope the trend is obvious now Because dictionary allows as O1 lookup we are able to quickly check if an elements ancestor exists If we continue this way the maximum length sequence will be stored in our counter by the end of the pass Also the fact that the sequence appears in sorted order is the only reason we can hope to find the ancestor But this is not true for the original problem Lets go back to the original problem with these observations and hope that some of them will apply Observations from our detour observationsfromourdetour One thing that continues to be true is that On implies that we must have an answer at every point of the pass If the array abruptly stops we must have an answer thats true for the subarray till then Another obvious but important observation is that we may have multiple potential sequences and we must track all of them One last observation is that we need a O1 lookup A way to check if our current element can be part of a sequence In the detour we saw this was easily accomplished by the dictionary because we knew for each potential sequence what the next element can be But we do not have this luxury now But lets explore this O1 lookup o1lookup O1 means that for every element we process we are allowed to up to a constant number of checks say 10 20 30 even but not anywhere close to n or even half of n Basically if we preset this number of checks to one constant value thats still O1 But what could we check We could check an element ahead behind We could check 20 elements ahead or behind But this check has to yield something This check has to tell us whether this current element can be part of a sequence Lets imagine some scenarios Scenarios scenarios Ancestor exists ancestorexists Say we put down an x for every element we encounter index 1 2 3 4 5 mark x Now imagine we do luckily encounter the next element in this potential sequence index 1 2 3 4 5 mark x x This current element can check behind it and find it nonempty This is huge This means we can tell that there is at least two elements in the sequence Ancestor arrives ancestorarrives This is the same thing but the ancestor arrives new ie 3 is already here index 1 2 3 4 5 mark x then 2 arrives index 1 2 3 4 5 mark x x Now 2 could check ahead and find a nonempty value to confirm that there are at least two elements in the sequence At least two can we do better atleasttwocanwedobetter Our O1 lookup now tells us that there are at least two elements in the sequence But can we somehow encode more information What if we store more than just a x Say we also store the number of elements in the sequence at this point index 1 2 3 4 5 3 arrives 1 Now we know that there is 1 element in the sequence potentially Now 2 arrives and sees that it has an element ahead which says its part of a sequence of length 1 Now 2 checks behind and sees nothing This means after 2 adds 1 to the sequence index 1 2 3 4 5 2 arrives 1 1 Now lets modify both 2 3 to store 2 as our sequence has increased index 1 2 3 4 5 2 arrives 2 2 This means every time we add an element to a potential sequence we increment the value of everybody involved in the sequence We could also store the maximum sequence value encountered in a separate variable If and when a new sequence has a higher length we update this maximum counter also This gives us a ready answer at every iteration Lets check if this works always Say this is the state index 1 2 3 4 5 value 2 2 1 Now 4 arrives index 1 2 3 4 5 value 2 2 x 1 It checks and behind finding sequences in both directions Its still possible to add the values ahead and behind then increment it by 1 giving us 2 1 1 4 as the new sequence length We update this for the entire sequence index 1 2 3 4 5 value 4 4 4 4 We also modify our maximum counter it was 2 before now 4 Our answer is readily available 4 Time complexity timecomplexity It seems our current approach works well We have assumed nothing about sortedness uniqueness etc and it still seems to work But wait Every time a new element is added to the sequence we are iterating over the sequence to modify the values This takes Os time for a sequence of length s every time This is not O1 at all O1 means no modification o1meansnomodification Every new element encountered can do O1 operation But by definition our sequence may be growing so we cannot operate on the whole sequence This means for a large part of our sequence theyll contain values that we stored a long time back ie stale values Stale values are obviously useless to update our findings So how what do we store If we cannot rely on ahead or behind lookups then we need something else to rely on A detour to our detour adetourtoourdetour Lets say xs denote the elements we have seen index 1 2 3 4 5 6 7 8 9 seen x x x There are only a handful constant slots here that actually change the existing sequences length marked by o index 1 2 3 4 5 6 7 8 9 seen o x x o o x o The good thing about these os is that they dont increase for an existing sequence Say 4 arrives index 1 2 3 4 5 6 7 8 9 seen o x x x o o x o The number of o slots remain the same just changed after a new inclusion Could we store these slots in a dictionary and check if a new element matches If it does we delete the element from the dictionary and add a new one or maybe two At the start of the pass there are no x values and no o values After the first x index 1 2 3 4 5 6 7 8 9 seen o x o We have two o values in the dictionary I imagine these o values as the shorelines of an island The island may be huge but the shoreline is limited lets not get into fractals When two such islands merge the shoreline actually reduces It goes from index 1 2 3 4 5 6 7 8 9 seen o x o x o to index 1 2 3 4 5 6 7 8 9 4 arrives o x x x o We have gone back to our dictionary lookup but with a few key modifications we store the slots where activity is triggered A new element arrives sequence grows or sequences merge Earlier we simply stored the last element encountered in the sequence which is useless when the sequence can arrive out of order the potential slots ie dictionary elements with o index key also contains the lengths of their islandssequences This means tracking the length is done in constant time rather than updating the whole sequence with a new value Just update the new o values being inserted this needs some changes based on a single o shared by multiple islands but it can be done Maybe an array is stored instead of integer An example anexample Lets try to manually solve an example with our new approach python import random array randomchoicelistrange0 20 for i in range0 10 printarray text 5 10 2 0 13 14 16 0 0 13 Say the given array is 5 10 2 0 13 14 16 0 0 13 index 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 5 arrives 1 x 1 10 arrives 1 x 1 1 x 1 2 arrives 1 x 1 1 x 1 1 x 1 0 arrives x 11 x 1 1 x 1 1 x 1 13 arrives x 11 x 1 1 x 1 1 x 1 1 x 1 14 arrives x 11 x 1 1 x 1 1 x 1 1 x x 2 Here we face a problem How do we update the other side of the island ie we need to update the element at 12 to 2 But how do we know what the other side is We know that there are only two sides of an island We could store for each shore for each o to hold the address of the other shore Note that we still only need to update one other shore ie a constant number of operations Lets redo this example with an object in each shore index 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 5 arrives max 0 l1 o6 x l1 o4 10 arrives max 1 l1 o6 x l1 o4 l1 o11 x l1 o9 2 arrives max 1 l1 o3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 0 arrives max 1 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 13 arrives max 1 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 l1 o14 x l1 o12 14 arrives max 2 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 l2 o14 x x l2 o12 16 arrives max 2 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 l2 o14 x x l2 1 o12 x 0 arrives max 2 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 l2 o14 x x l2 1 o12 x 0 arrives max 2 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 l2 o14 x x l2 1 o12 x 13 arrives max 2 x l11 o 3 x l1 o1 l1 o6 x l1 o4 l1 o11 x l1 o9 l2 o14 x x l2 1 o12 x Our final max length is 2 as the max counter says But we just found another flaw in our approach When we encounter an element that falls in the shore we need to know which direction to propagate the shore This can also be stored in the shores object Lets say we just 1 for propagating behind and 1 for propagating ahead A simpler way asimplerway Of course we could code this up and see how it works We might even encounter more issues But one issue with our current approach is the number of different cases to handle two shores merging direction of the shore a pointer to the other shore The implementation would look quite monstrous But lets take the intuition we have gained and see if we can improve the approach Intuitions intuitions Our current understanding is that elements form islands The islands merge when they come in contact This creates bigger islands Our task is to find the size of the biggest island Our first approach ourfirstapproach At first we tried storing the sequence length in every element of the sequence Say the sequence length is visualized vertically At first only 2 as arrived x value 1 index 1 2 3 4 5 Then 3 arrives x x x x value 2 2 index 1 2 3 4 5 Finally 4 arrives x x x x x x x x x value 3 3 3 index 1 2 3 4 5 Every time we modify the values we are actually building a block thats one dimension higher This is costly because we have create to go from 1x1 to 2x2 to 3x3 adding more and more blocks every time This type of visualization can help us understand why this operation was costly We can also think of other ways to indicate the sequence size At this point we are just doodling Maybe we dont build a full block just a pyramid x x x x x value 3 3 3 index 1 2 3 4 5 Again we need to know a symmetric way to build this Which means we may have to know where the two ends of the sequence is all the time A different data structure adifferentdatastructure One issue that keeps coming up is keeping info on both sides of the sequence where new elements could come We tried to store only the end points in a dictionary But we have lots of cases to handle when the sequence changes or grows or when merges happen What if we dont keep the endpoints separate Is there a way we could refer to the same sequence by either end point or start point We can assume this there is only one sequence between a unique pair of start end start is never equal to end start end are both integers But wait Merge condition when two sequences merge on the arrival of a new element Then a given key could find a sequence behind it and ahead of it So mapping a key to one sequence is not possible Only a pair of start end are unique to a single sequence So this wouldnt work Intervals intervals What if we simply refer to each island by only its start and end points Like an interval 1 2 3 4 5 6 x x x x x we would have start end 1 2 4 6 The longest sequence is the one with the smallest start a and the largest end b At this point this seems familiar our problem consists of a set of intervals ab These intervals are changing as new elements appear Intervals can grow sometimes merge We are constantly tracking the largest interval as our answer The crux of this is finding a constant time way to insert new element into its correct interval from a set of intervals Is this even possible Given a set of intervals even sorted we still have to check at least a few of them to find the right place Maybe binary search could do this in log n but constant time No way We could still go ahead and flesh out the details using this approach Not On but okay notonbutokay Say we maintain an array a linked list of intervals We make sure that every new element is inserted using binary search to ensure sortedness of this array Thankfully there will be no overlaps of intervals because the intervals merge instead of staying overlapped this should reduce some complexity Inserting in the middle will be Olog n if we use linked list instead of array Every time we insert we also check if the recently modified interval can merge with the next or previous interval If it can we do merge in O1 time Say we iterate through the final linked list of intervals to find the largest On time In total this would be On log n n Nothing but dictionaries nothingbutdictionaries Out of all the approaches we have explored dictionaries are the only data structures that gives us O1 lookup We may have to go back and clean up our dictionary approach try coding it up and see where that goes Our issue with the dictionary approach were the number of cases to handle in a single dictionary Instead lets simplify this Well have two dictionaries one with the start element as key and one with the end element as key Start element dict start startelementdictstart This will have only one value the end point So starte has the start point of a sequence that ends at e If starte does not exist then there are no sequences that end at e End element dict end endelementdictend This will have only one value the start point So ende has the end point of a sequence that starts at e If ende does not exist then there are no sequences that start at e Some properties to maintain somepropertiestomaintain startende e The end point of a sequence that starts at e is ende Then the start point of a sequence that ends at ende is e Remember we have unique sequences only endstarte e The start point of a sequence that ends at e is starte Then the end point of a sequence that starts at starte is e Remember we have unique sequences only The cases thecases An element e arrives e1 is an end point anelementearrivese1isanendpoint This means we assign the value of ende1 to ende and remove ende1 But wait there is some element in start that has a value of e1 We have to change that to e ie we need to change startende1 e An element e arrives e1 is a start point anelementearriveseplus1isastartpoint This means we assign the value of starte1 to starte and remove starte1 But wait there is some element in end that has a value of e1 We have to change that to e ie we need to change endstarte1 e An element e arrives e1 is an end point AND e1 is a start point anelementearrivese1isanendpointandeplus1isastartpoint At the end of this we need we want a merged interval Where does the sequence start when it ends at e1 It starts at starte1 Where does the sequence end when it starts at e1 It ends at ende1 After our operations in this iteration we want endstarte1 ende1 and startende1 starte1 This is the crux of it Reading it and drawing it out helps in understanding Before this iteration we have start value e1 a and end value e1 b After this iteration we need start value b ende1 a starte1 and end value a starte1 b ende1 and we also delete starte1 no sequence ends at e1 delete ende1 no sequence starts at e1 Summary summary All of the above lookups and modifications are O1 We have avoided nested dictionaries and arrays by simply using two dictionaries which makes the code cleaner and clear Note that we still have not tracked the largest sequence yet For now lets skip it and implement our current understanding This is a quick implementation of this approach python array 6 4 7 8 8 7 7 5 6 9 0 7 7 7 3 8 1 9 5 5 start end for e in array if e1 in end and e1 in start startende1 starte1 endstarte1 ende1 endpope1 startpope1 elif e1 in end ende ende1 endpope1 elif e1 in start starte starte1 startpope1 else create starte e ende e starte e ende e length 0 seqstart None seqend None for ab in enditems if ba length length ba1 seqstart a seqend b printBest sequence of length d from d to d length seqstart seqend text Best sequence of length 4 from 3 to 6 This is wrong The implementation does not work We missed a crucial factor duplicate elements When duplicate elements arrive we may end up adding them again as a 1length sequence but this violates the nonoverlapping structure One way to correct this is to maintain a dictionary of processed elements and skip them when they arrive again python array 6 4 7 8 8 7 7 5 6 9 0 7 7 7 3 8 1 9 5 5 start end processed for e in array if e in processed continue if e1 in end and e1 in start startende1 starte1 endstarte1 ende1 endpope1 startpope1 elif e1 in end ende ende1 startende1 e endpope1 elif e1 in start starte starte1 endstarte1 e startpope1 else create starte e ende e starte e ende e processede 1 length 0 seqstart None seqend None for ab in enditems if ba1 length length ba1 seqstart a seqend b printBest sequence of length d from d to d length seqstart seqend text Best sequence of length 7 from 3 to 9 Testing testing Lets now try our implementation against the exhaustive test cases of leetcodehttpsleetcodecomproblemslongestconsecutivesequencedescription It seems to pass for the one test input given Before we submit we could generate a lot of test cases randomly and paste it in the custom input to weed out any mistakes Lets use the constraints given in leetcode 0 numslength 109 numsi Runtime 60 ms faster than 3096 of Python3 online submissions for Longest Consecutive Sequence Memory Usage 153 MB less than 1887 of Python3 online submissions for Longest Consecutive Sequence We could definitely remove the loop at the end and instead maintain a largest sequence length Improvements improvements Maintaining the maximum length in the same iteration maintainingthemaximumlengthinthesameiteration This is fairly simple For each case the length calculation changes For the merge case we calculate ende1 starte1 1 For the append case we calculate e starte1 1 For the prepend case we calcualte ende1 e 1 For the new sequence case the length is 1 If we do change the maximum length then we also change the start and end points using which we calculate the length above python array 6 4 7 8 8 7 7 5 6 9 0 7 7 7 3 8 1 9 5 5 start end processed maxlength 0 seqstart None seqend None for e in array if e in processed continue if e1 in end and e1 in start startende1 starte1 endstarte1 ende1 if ende1 starte1 1 maxlength maxlength ende1 starte1 1 seqstart starte1 seqend ende1 endpope1 startpope1 elif e1 in end ende ende1 startende1 e if ende e 1 maxlength maxlength ende e 1 seqstart e seqend ende1 endpope1 elif e1 in start starte starte1 endstarte1 e if e starte1 1 maxlength maxlength e starte1 1 seqstart starte1 seqend e startpope1 else create starte e ende e starte e ende e if 1 maxlength maxlength 1 seqstart e seqend e processede 1 length 0 seqstart None seqend None for ab in enditems if ba1 length length ba1 seqstart a seqend b printBest sequence of length d from d to d maxlength seqstart seqend text Best sequence of length 7 from 3 to 9 Lets see how we do Runtime 60 ms faster than 3096 of Python3 online submissions for Longest Consecutive Sequence Memory Usage 154 MB less than 1652 of Python3 online submissions for Longest Consecutive Sequence Not much difference Understanding the best solution understandingthebestsolution At this point lets check the best solution as per leetcode and see how we can connect our understanding to it python class Solution def longestConsecutiveself nums longeststreak 0 numset setnums for num in numset if num 1 not in numset currentnum num currentstreak 1 while currentnum 1 in numset currentnum 1 currentstreak 1 longeststreak maxlongeststreak currentstreak return longeststreak Handling duplicates handlingduplicates Simply calling set on the array removes duplicates python numset setnums The loop theloop Again the loop is over each element in the set In our case we go over array followed by an if block which is equivalent to this loop The set theset The biggest difference is that there is only one set in this solution They check if previous element ie num 1 is in the set This is equivalent to us checking if e1 is present in end If it is they do no processing This is the important difference from our solution They simply continue on to the next element Checking ahead no checking behind checkingaheadnocheckingbehind This is no checking behind in their implementation They have fixed the counting direction as forwards only So we only count when we encounter the start of a sequence How do we know the start of a sequence By checking if previous element is in the set This is extremely elegant because there is no additional storage involved There is a single set which contains all the necessary elements to check We simply go through them checking if we have encountered the start of a sequence Counting counting If we have encountered the start of a sequence we start counting ie keep incrementing counter as long as next element is also in the set If by the end we have a better length we update our maximum length variable Takeaways and observations takeawaysandobservations A loop inside a loop is not necessarily bad if its only triggered very sparingly Thats why this solution is still On even with the loop inside Using additional memory also means we are handling the additional overhead of seen and not seen elements By using a single set and not having the overhead memory removes a lot of code that handles these cases Clever elimination The use of a set allows for O1 lookup of ancestors We ignore anything that is not the start of a sequence Using a set Forming a set from an array is arguably the biggest learning of all I think the conversion itself would take On time but this is okay I definitely didnt think this preprocessing would make things faster In fact I was deadset on iterating the array"}]