[{"title":"Fourier Analysis","href":"/posts/20200524030032-pubnotes_ddse_fourier","content":"This is for my notes for Prof Steve Bruntons video lectures of Fourier AnalysishttpswwwyoutubecomwatchvjNC0jxb0OxElistPLMrJAkhIeNNT5FXh3Oy0Y4LTj0Oxo8GqsC Real domain to complex domain realdomaintocomplexdomain The first confusion for me was the shift in the sum limits from real domain to complex domain The real fourier series is expressed as sumk0infty fraca02 ak cos kx bk sin kx The intuition here is that harmonics can express any function The constant term accounts for all the different initial phase shifts of each of these harmonics The mathematical property of these trigonometric terms sinkx coskx is that they are orthogonal Their inner product intpipi fx gx dx 0 To verify The conjugation on the second term is irrelevant for real valued functions fg can be any sinkx coskx term This lecturehttpsocwmiteduresourcesres18008calculusrevisitedcomplexvariablesdifferentialequationsandlinearalgebrafall2011studymaterialsMITRES5F185F0085FpartIII5Fsol08pdf goes into detail about each combination The intuition is that intpipi sin ax cos bx is an odd function so integrating over this particular limit will be zero But when ab int sin2 ax int cos2 ax are reduced using trigonometric identities to a constant plus a periodic function and the constant part evaluates to pi All of the above is for the real domain In complex domain the fourier series becomes sumkinftykinfty ck eikx where ck in mathbbC The sum starts from negative infinity now unlike the real domain Oscillations in real domain rotations in complex domain oscillationsinrealdomainrotationsincomplexdomain In real domain the eigenfunctions are either sin kx or cos kx This means that sinkx and sinkx are not orthogonal This can be verified by the inner product which evaluates to pi So negative k in the real fourier series redundant as the eigenfunctions of k 0 already include the k 0 part too Another way to think about this is phase shift k 0 terms All the phase shift terms are already accounted for by a0 But in complex domain the eigenfunctions represent a rotation at different frequencies and starting points rather than oscillation k 0 represents an anticlockwise rotation This videohttpswwwyoutubecomwatchvspUNpyF58BY helps In this case k 0 functions The computation of the inner product confirms the same of course TODO The complex conjugation of the second term in the inner product is equivalent to a direction change of rotation I know that the integration of a complex function is related to contour integration but I dont see how the inner product in the complex domain represents closeness of two rotations Yet to read thishttpwww1spmsntuedusgydchongteaching085Fcontour5Fintegrationpdf Backlinks backlinks DDSE compressed sensing with DFT Now we compute the FFT and confirm with the result previously Scaling Learning Algorithms towards AI It seems theres also a relation to the Fourier Analysis in terms of layers The authors suggest that functions with high frequencies in their fourier spectrum cannot be represented with two layer architectures Linial et al 1993 is the source of this finding This is extremely interesting but also a rabbit hole Is it worth going into this now Ill have to come to back to this The summary from this section is that shallow architectures are poor at describing more complex functions"},{"title":"Compressed Sensing with the Fourier Ensemble","href":"/posts/20200611111551-compressed_sensing_with_the_fourier_ensemble","content":"Im reading the topic of compressed sensing and I find it extremely dense with a lot of historical perspective This videohttpswwwyoutubecomwatchvaHCyHbRIz44 is a great start to the topic Reproducing the example reproducingtheexample The example at 1405 in the videohttpsyoutubeaHCyHbRIz44t846 uses a DCT The cosamp function used below is from this repositoryhttpsgithubcomavirmauxCoSaMP The code here is slightly modified from the jupyter notebook available herehttpdatabookuwcom The sample audio signal thesampleaudiosignal python stx 0 end 1 n 4096 Fs int nend stx from math import pi from numpyfft import rfft rfftfreq t nparangestx end 1Fs x npcos2 97 pi t npcos2 777 pi t f rfftx psd ffn freqs rfftfreqn Fs pltclose pltfigure pltplotfreqs f r fint npfloorabsf filename audio reconstruction compressed sensing pltsuptitlefilename pltsavefigffilenamepng pltclose xfft npfftirfftf pltsaveinverse fourier check compressed sensing lambda pltplott xfft r pltsavezoom in inverse fourier check compressed sensing lambda pltplott xfft r pltxlim025033 text homeplocallibpython36sitepackagesnumpycoreasarraypy85 ComplexWarning Casting complex values to real discards the imaginary part return arraya dtype copyFalse orderorder The code thecode python p 128 from scipyfftpack import dct idct In npdiagnponesn Psi dctIn axis0 Psi splinalgdftnreal pltsavePsi matrix heatmap cosamp lambda pltimshowPsi cmaphot interpolationnearest idxs nprandomrandp nastypeint y xidxs pltsavemarked measurements of original signal for compressed sensing cosamp lambda pltplott x tidxs y rx pltxlim025 033 Theta Psiidxs pltsaveTheta matrix heatmap cosamp lambda pltimshowTheta cmaphot interpolationnearest xest cosampTheta y 3 Score estimation printfScore estimation nplinalgnormx xest nplinalgnormx pltsaveresult heatmap dct cosamp lambda pltplott xest xdct idctxest pltsavecompare original signal with recovered signal from cvxopt dct cosamp lambda pltplott xdct r t x k pltxlim025 033 text main35 FutureWarning rcond parameter will change to the default of machine precision times maxM N where M and N are the input matrix dimensions To use the future default and silence this warning we advise to pass rcondNone to keep using the old explicitly pass rcond1 Score estimation 10002591450121945 While I can reproduce the result from the video I find the selection of basis confusing If the random drawing of measurements is from the DCT matrix then is it equivalent to a measurement in the frequency domain But the code is clearly measuring from the time domain samples For example could I use the Fourier basis and achieve the same results Simply changing from DCT to DFT does not work I tried Interestingly I recover back a signal which has the correct frequency spikes but very low energy So the resulting estimation is scaled down by 3 orders but the frequency content matches Back to basics backtobasics From the original workhttpsarxivorgabsmath0410542 Sec 11 the authors define FOmega f hatfk k in Omega The Omega is a set of frequencies sampled uniformly in random The reconstruction g is obtained by min g l1 min Sigmat in mathbbZn gt subject to hatgk hatfk forall k in Omega In the above signal the sparsity is expected in the frequency domain ie we minimize the l1 norm in the frequency domain The equivalent problem formulation theequivalentproblemformulation min hatg l1 subject to gt ft forall t in T and the result is highly probable to be bhatf The Theta matrix thethetamatrix FOmega now is the random drawing of rows from the ifft Using IDFT as Psi usingidftaspsi Psi can also be created from applying the inverse fft to a identity matrix since its an unitary operation The idea is to randomly sample rows of the Psi matrix and create a theta python random indices of the DFT between 0 and n 4096 p 128 idxs nprandomrandp nastypeint In npdiagnponesn idft npfftifftInreal python Theta idftidxs pltsaveTheta heatmap for fft basis lambda pltimshowTheta cmaphot interpolationnearest The measurement y is a sparse signal taken at random indices of the actual x beginsrc python session compressedsensing measurement is random sampling at the above indices y xidxs Using file20200608115955convexoptimizationorgconvex optimization We can use convex optimization to solve for the sparsest s sres optminimizelambda e nplinalgnorme 1 x0 npzerosn constraints type eq fun lambda e Theta e y sres cosampTheta y 10 pltsaveresulting sparse in FFT basis lambda pltplotsres pltimshownpmatrixsres cmaphot interpolationnearest xfft npfftifftsres pltsavecompare original signal with recovered signal from cosamp dft lambda pltplott xfft r t x k pltxlim025033 text main35 FutureWarning rcond parameter will change to the default of machine precision times maxM N where M and N are the input matrix dimensions To use the future default and silence this warning we advise to pass rcondNone to keep using the old explicitly pass rcond1 homeplocallibpython36sitepackagesnumpycoreasarraypy85 ComplexWarning Casting complex values to real discards the imaginary part return arraya dtype copyFalse orderorder Results results So drawing from the inverse fft matrix does work The recovered basis also shows the spikes at the right frequencies Unresolved confusions unresolvedconfusions How is it possible to draw from the DCT matrix while measuring the time domain samples Using DCT as Phi does lead to correct recovery of the original frequencies so theres no flaw in the method Units of the recovered DCT basis yaxis"},{"title":"lassoPlot in python","href":"/posts/20200624220142-lassoplot_in_python","content":"A comparison of python vs matlab lassoPlot in matlab lassoplotinmatlab In matlab lasso regressions MSE is easily visualized using the command lassoPlot The following is from Prof Steve Bruntons uwdatabook pagehttpuwdatabookcom In the zip file the exact file is CH03SEC052LASSOm The most relevant parts to create the plot are picked out here nil A randn10010 Matrix of possible predictors x 0 0 1 0 0 0 1 0 0 0 Two nonzero predictors b Ax 2randn1001 Observations with noise xL2 pinvAb XL1 FitInfo lassoAbCV10 lassoPlotXL1FitInfoPlotTypeCV save See output below Equivalent command in python equivalentcommandinpython In python the function calls are completely different In fact there no direct way to mention 10fold cross validation like in lassoAbCV10 This answerhttpsstackoverflowcomquestions12746479whyarelassoinsklearnpythonandmatlabstatisticalpackagedifferent2045740220457402 shows how to create the KFold cross validation model first and then use it in LassoCV In the previous matlab code sample I saved the variables so as to recreate the exact plot in python python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt from scipyio import loadmat import sklearnlinearmodel no effect on algorithm plotting helper function def pltsavetitle funcarr Helper function for laying out plots quickly title plot title titlepng is the file name given to savefig funcarr array of functions one each executed for a column of subplots each function can include plotting axes changes xlim ylim calls Basically anything specific to that subplot can be done in that function pltclose fig axs pltsubplotslenfuncarr 1 squeezeFalse constrainedlayoutTrue for ax func in zipaxsT0 funcarr funcax pltsuptitletitle pltsavefigftitlepng pltclose alldata loadmatmatlabmat this represents the measurement A alldataA x alldatax b alldatab with cross validation kf sklearnmodelselectionKFoldnsplits10 shuffleTrue printsklearnmodelselectionKFolddoc cv sklearnlinearmodelLassoCVcvkf normalizeTruefitAb msepath contains the errors for each fold alpha pair we want to plot the variation of errors in each alpha across the folds ie alpha on xaxis the y spread shows the error spread eps 001 the smaller it is the longer is the path alphaslasso coefslasso sklearnlinearmodellassopathA b epseps mean lambda e sumelene minmses mincvmsepathi for i in range0 lencvalphas maxmses maxcvmsepathi for i in range0 lencvalphas meanmses meancvmsepathi for i in range0 lencvalphas devmses npstdcvmsepathi for i in range0 lencvalphas idxalphaofminmse npwherenpisclosecvalphas cvalpha00 minmseoveralphas meancvmsepathidxalphaofminmse standard deviation of the error at the best fit MSE dev devmsesidxalphaofminmse printnprounddevmses2 printfmeanmses nproundmeanmses 2 printfdev nprounddev2 find closest alpha for which mean mse is close to dev idxssparsealphacandidates npwhere npisclose meanmses nponeslikemeanmses minmseoveralphas dev atol01 printmeanmses minmseoveralphas dev printfidxssparsealphacandidates idxssparsealphacandidates idxsparsestalpha idxssparsealphacandidates00 printidxsparsestalpha sparsestalpha cvalphasidxsparsestalpha printfcvcoefcvcoef printfsparsestalpha sparsestalpha printfoverfitalpha cvalphasidxalphaofminmse without cross validation at sparsest alpha clf sklearnlinearmodelLassoalphasparsestalpha clffitA b printfclfcoef clfcoef identify the active clf coefs idxactive npwherenplogicalnotnpiscloseclfcoef npzeroslikeclfcoef printnpisclosedoc printfidxactive idxactive Aactive npzeroslikeA Aactive idxactive A idxactive printnpshapeAactive printnpshapeb xestimateactiveonly nplinalgpinvAactive b printnproundx xestimateactiveonly 2 pltsavepython lasso demo LassoCV show error spread lambda ax axplotcvalphas m d2 for md in zipmeanmses devmses o axplotcvalphas m d2 for md in zipmeanmses devmses o axplotcvalphas meanmses o axplotcvalpha minmseoveralphas x axplotcvalphasidxsparsestalpha meanmsesidxsparsestalpha kx axinvertxaxis axsetxscalelog axplota a m d2 m d2 y for a m d in zipcvalphas meanmses devmses pltsavepython lasso demo lambda ax axplotcvcoefo label10fold cv axplotx x labeltrue values x axplotclfcoef o labelno cv alpha at sparsest axplotl2res o labell2 axplotxestimateactiveonly o labell2 regression correction axplotnplinalgpinvA noise s labelinvA noise axlegend text homeplocallibpython36sitepackagessklearnutilsvalidationpy73 DataConversionWarning A columnvector y was passed when a 1d array was expected Please change the shape of y to nsamples for example using ravel return fkwargs 251 25 245 24 236 233 231 229 228 226 225 224 224 224 224 224 224 225 225 226 227 227 228 228 229 23 23 231 232 233 234 235 236 237 238 238 239 24 24 241 242 242 243 244 245 245 246 247 248 248 249 249 25 25 251 251 251 252 252 252 253 253 253 254 254 254 254 254 255 255 255 255 255 255 256 256 256 256 256 256 256 256 256 256 256 257 257 257 257 257 257 257 257 257 257 257 257 257 257 257 meanmses 554 55 543 536 53 526 523 52 518 516 514 51 508 506 504 503 502 501 501 502 504 506 508 511 514 517 52 522 525 527 529 532 534 536 539 541 543 544 546 548 549 551 552 554 556 557 559 56 561 562 563 565 565 566 567 568 569 57 57 571 572 572 573 573 574 574 575 575 575 576 576 576 576 577 577 577 577 578 578 578 578 578 578 578 579 579 579 579 579 579 579 579 579 579 579 579 58 58 58 58 dev 225 idxssparsealphacandidates array dtypeint64 Traceback most recent call last File line 1 in File tmpbabelNg5Fd9pythontkuHOj line 69 in idxsparsestalpha idxssparsealphacandidates00 IndexError index 0 is out of bounds for axis 0 with size 0 Important differences importantdifferences Not the same cost function notthesamecostfunction The python function call LassoCV and matlab function call lasso do not solve the same optimization problem LassoCV solves a normalized version of the problem by diving the cost function by the number of samples This changes the resulting penalty values being used in the two versions In matlab using lambda in ab is equivalent to using lambda in na nb in python See matlab lasso documentationhttpswwwmathworkscomhelpstatslassohtml end of the page Compare to scipy LassoCV documentationhttpsscikitlearnorgstablemodulesgeneratedsklearnlinear5FmodelLassoCVhtml This is visible from the xaxis of the plots and you see a they are off by a factor of 100"},{"title":"Algo walkthrough: swapping elements to find max score","href":"/posts/20201122105230-algo_walkthrough_swapping_elements_to_find_max_score","content":" Problem statement problemstatement Given an array of size n the beauty of this array is defined as sum of absindexvalue for each element Swapping two elements of this array what is the maximum beauty possible Initial setup initialsetup The initial setup consists of an unsorted array Each element has a score defined as the absolute value of the difference the elements value and its index The absolute value theabsolutevalue The metric for each element gives us a sense of displacement For example if an element at index 4 is of value 10 the score for this is 6 If the element was at index 5 and of value 11 the score is still 6 This suggests that we are to think in terms of displacement Displacement from what displacementfromwhat Here we can think of the two parts in the difference We have an index and we have a value We could think of the elements score as its displacement Currently the element is at index i The displacement is i e So originally it was at e A new of approaching this anewofapproachingthis Imagine we have a row of bins We are randomly filling its slots with the index of the slot So we can choose to fill slot 2 with the value 2 In fact we are allowed to fill the same slots multiple times forget for now how this will be represented in an array as arrays only allow one value in a slot The bins are now filled Suddenly each of the elements start moving They can move to the left or to the right They can move at different speeds After a while they all stop Again assume here that we now have a row of elements none overlapping by some miracle So initially we may have multiple elements in the same bin but now that they have randomly moved around they now occupy unique slots Now we can represent them in an array The score thescore Now we can think of the score for each element as the amount it has moved from their original bins The problem theproblem The question for us is to swap the position of two elements in this array so that we increase the total score of the the whole array This might have multiple possible solutions so instead we are looking for the pair of elements when swapped will increase the total score the most Details of the question detailsofthequestion Increasing the score of one element increasingthescoreofoneelement Is there a way to increase the score of just one element The score for an element is i e The elements value e is fixed The i can be changed but not in isolation Because we are using an array we are assuming that the indices are running contiguously So every index must be occupied by some valid element and there cannot be any gaps So we definitely need another element which can take this elements place Swap swap This means swapping is the minimal atomic way of changing the score of the array During a swap we change the indices of two elements in the array so only the scores of these two elements change By changing these we are changing the total score Useful swaps usefulswaps Can we change any two elements to achieve a change in score In other words is there a way to reduce our search space Model 1 model1 Say we represent an element as a tuple with its index and the value i e Now lets take two elements i1 e1 i2 e2 Their scores are i1 e1 i2 e2 Now lets define what swapping means When we swap we actually swap the indices of the elements leading to i2 e1 i1 e2 The scores now become i2 e1 i1 e2 What is the increase in the score i2 e1 i1 e2 i1 e1 i2 e2 At this point we have to take different cases to analyze i1 e1 i2 e2 i1e1i2e2 In this case the score is simply i1 e1 i2 e2 before swapping and i2 e1 i1 e2 after swapping Here again there can be multiple cases i2 i1 e1 i1 e2 i2i1e1i1e2 In this case the change in score becomes zero remove the modulus and check for yourself i2 i1 e1 e2 i1 i2i1e1e2i1 In this case the change in score is i2 e1 e2 i1 i1 e1 i2 e2 e2 i1 i1 e2 2e2 i1 Model 2 model2 As we see it definitely helps to analyze elements as a tuple We were able to rule out some parts of the search space But one issue is the number of different cases we could have It might help to think of the elements as points in space Lets try thinking of indices running along the Yaxis and plot the points as lengths running along Xaxis An example can help Say the array 4 5 0 1 python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2darray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray pltplotlistrange0 lenarray listrange0 lenarray b for idx e in enumeratearray pltplotrangemine idx maxe idx 1 idx 1 absidx e k axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltsavefigfilename pltclose array 4501 plotarrayas2darray swapssamplesmallarraypng length 20 array randomchoicelistrange0 length for i in range0 length plotarrayas2darray swapssamplearrayplotpng The blue line is the xy line ie indexvalue line Some immediate observations Each elements score is the black line Imagine moving the element to a different index while retaining the Value and try to see how the black lines length changes When elements are swapped their Value remains the same but their score changes because the distance to the blue line changes Fallacy Elements on left of the blue line seemingly have no benefit in moving down as this reduces the score But this is not true It seems this way because of the example See next example Fallacy Elements on right of the blue line seemingly have no benefit in moving up as this reduces the score But this is not true It seems this way because of the example See next example Two elements whose score lines ie the black lines are overlapping have no benefit swapping as this reduces their scores Its time to expand our example to check our observations A larger array example to see more observations alargerarrayexampletoseemoreobservations We have seen certain observations in the previous visualization Lets see how many of them still remain valid with a larger example oxhugoswapssamplearrayplotpng Overlapping score lines before and after swap overlappingscorelinesbeforeandafterswap This definitely seems to hold true Notice the elements at index 6 and 8 They are on the same side of the blue line They have an overlapping score line On swapping whatever is gained by 6 is lost by 8 leading to no change in the total score Notice the elements at index 18 and 17 They are on opposite sides of the blue line They have a small overlap On swapping this overlapping part is lost by both sides leading to decrease in scores for both ie decrease in total score Notice the elements at index 5 and 7 They are on the same right side of the blue line They have an overlapping score line On swapping whatever is gained by 5 and 7 leading to no change in the total score Reverse of above cases reverseofabovecases For each of the cases above notice that the reverse causes the opposite effect for a zero change in score the opposite is also zero change in score The second case is interesting The decrease in score on removal of the overlap implies that causing an overlap increases score Well come back to this Fallacy No benefit in crossing the blue line fallacynobenefitincrossingtheblueline This seems to be a fallacy in our previous observation Notice how element at 15 could in fact move down all the way crossing the blue line and achieve a better score assuming we can have a partner who wants to move up Summary summary On swapping one element moves up one element moves down We want an increase in score and we have seen thatorg088732c causing an overlap increases score We also see that an element can cross the blue line to increase its own score Together with the above two observations we can say that if we pick any two nonoverlapping score lines and swap them there is a definite increase in total score Take 4 and 14 On swapping we get an increase in score which is equal to 2 x gap between the lines Take 6 and 9 On swapping we get an increase in score which is equal to 2 x gap between the lines Strategy strategy We have reduced the search space to only nonoverlapping elements This is still a large search space of say s and our current understanding is to iterate over s2 options and find the maximum increase Is there a way to reduce our search space further Model 3 model3 From our observations in Model 2 we know that any two elements whose score lines dont overlap can be swapped to achieve an increase in total score With this in mind we can approach this problem with a fresh perspective Lets start by looking at each element as simply an interval Think of a horizontal line with a start and an end Out of many such horizontal lines we are looking for two that maximize spacing between them Visualizing intervals visualizingintervals Lets try to see this in action python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2dintervalsarray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray for idx e in enumeratearray pltplotrangemine idx maxe idx 1 idx 1 absidx e k linewidth10 if e idx pltscattere e markers colork s100 axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltgrid pltsavefigfilename pltclose array 4501 plotarrayas2dintervalsarray swapsmodel3samplesmallarraypng length 20 array randomchoicelistrange0 length for i in range0 length plotarrayas2dintervalsarray swapsmodel3samplearrayplotpng text Python 385 default Sep 7 2020 110458 GCC 750 on linux Type help copyright credits or license for more information pythonel native completion setup loaded oxhugoswapsmodel3samplearrayplotpng Note that this is just Model 2 without the blue line Our next step is to form a strategy to eliminate our search space fast Transitivity transitivity One way to eliminate parts of search space is by using a transitivity property We need a way to show that two elements e1 e2 cannot form the optimal swap then we can eliminate a set of elements related to e1 and e2 For example 0 and 5 are already overlapped So swapping them only leads to nonoverlap and a reduction in score Note that 1 2 3 4 are all elements that have a starting point thats before 5 but after 0 So if 5 overlaps with 0 then all these elements 1234 also will overlap with 0 We can eliminate them for pairing with 0 without checking Another example Note that 0 and 18 overlap but 0 and 17 do not So this is not about the indices but rather the starting point Here even though 18 has a higher index than 17 the starting point 17 is after 18 So we cannot eliminate 17 based on test of 0 and 18 But we can eliminate pairing 0 with 7 6 5 4 3 2 as they all have starting points before 18s starting point Achieving transitivity achievingtransitivity Transitivity is useful but we need an apt data representation to exploit this In our current representation we just discussed how we are unable to eliminate 017 by testing for 018 This is because the starting points are not related to the indices This gives us a clue Lets try sorting our intervals based on the starting point Remember that we are not sorting the original array that would defeat the purpose as we are only supposed to make one swap and sorting it is a lot more swaps We are simply changing our visualization of the original array to look for any patterns python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2dintervalsarray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray pairs for idx e in enumeratearray pairsappendmine idx maxe idx sortedstart sortedpairs keylambda e e0 for idx e in enumeratesortedstart pltplotrangee0 e1 1 idx 1 abse1 e0 k linewidth10 if e0 e1 pltscatteridx idx markers colork s100 axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltgrid pltsavefigfilename pltclose plotarrayas2dintervalsarray swapsmodel3sortedsamplearrayplotpng We now have a way to tell which two elements are most apt for pairing It seems the elements picked one each from the two extremes would give us a good spacing But is it the best May not be We can see element at 1 not element at 0would be a better choice to pair with 19 We can see why because 0 ends before 1 does and so has more spacing with 19 Note that now have transitivity almost We can start with the extremes and say for sure that these may be our best bet Close to the end closetotheend Our observations from previous section show that a good starting point is crucial We say good to mean smallest In our representation we are pairing a small starting point element with a large starting point element But our fallacy was coupling starting and ending points Ending point value is completely independent from starting point value for each element So what is a desirable ending point value Wed like it to be as close to the starting point as possible This makes the metric for ending point dependent on the elements starting point In fact this is simply the score for each element So for the first element of the swap we are looking for an element that starts early and last a small score For the other element of the swap we only care that it starts as far as possible and not about the score because the gap doesnt depend on where the second element ends This gives us an idea What if we also sort based on the end point python import numpy as np import matplotlib matplotlibuseAgg import matplotlibpyplot as plt import matplotlibticker as ticker import random def plotarrayas2dintervalsarray filename ax pltgca pltscattere idx s100 markers colork for idx e in enumeratearray pairs for idx e in enumeratearray pairsappendmine idx maxe idx sortedstart sortedpairs keylambda e e1 for idx e in enumeratesortedstart pltplotrangee0 e1 1 idx 1 abse1 e0 k linewidth10 if e0 e1 pltscatteridx idx markers colork s100 axyaxissetmajorformattertickerFormatStrFormatterd axxaxissetmajorformattertickerFormatStrFormatterd tickerMultipleLocatorMAXTICKS 2000 axyaxissetmajorlocatortickerMultipleLocatorbase1 axsetylabelIndex axsetxlabelValue pltgrid pltsavefigfilename pltclose plotarrayas2dintervalsarray swapsmodel3sortedendsamplearrayplotpng For comparison lets compare the two representations sort based on start point sort based on end point We are looking for two elements here one that appears relatively close to the bottom in both these plots one that appears relatively higher to the top in both these plots For each element we could sum up their index in both the arrays We can then pick the element with the smallest sum and the element with the largest sum This would give us the correct answer But is there a faster way to find these elements without having to sort the array twice Faster way to find these elements fasterwaytofindtheseelements Say we allow for one sorting Lets sort by start point oxhugoswapsmodel3sortedsamplearrayplotpng Lets try to find a suitable element to be the left side of the swap Well refer to the elements as a tuple of their start point end point Our first option is 0 15 So this element ends at 15 Our best gap at this point starts at 15 We can rule out any element that starts after 15 because they will only be worse ie the gap would start later and not useful So we have already removed part of our search space All elements with start point greater than 15 can be ruled out Lets move on to the next option in our search 12 This gives a gap that starts at 2 which is obviously better our earlier option of 15 And now we can rule out any element that has a start point greater than 2 We have now removed a huge percentage of our search space now Our next option is 13 which means a gap that starts at 3 But we already have a better option that starts at 2 Our options after this all start at 2 which we have already ruled out We are done But this is just half the battle We have only found the left side of the swap We still need the right side of the swap For the right side element our criteria is finding the largest start point ie the gaps end But this is straightforward We can simply take the last element of the current sorted array We dont care about where this element ends because the gap size doesnt change Edge cases edgecases Okay so we have a full solution to find the left and the right side of the swap What happens if we find that these two elements are overlapping This means there are no elements that can be swapped to achieve a better score Our criteria was maximizing the gap when we found these elements But the best gap we can achieve is not positive Tests tests Well write our strategy as code and check if it works against a random array Just to be sure well also compute the n2 solution ie brute force by checking each element with every other element python array randomchoicelistrange0 1000 for i in range0 1000 pairarray mine idx maxe idx for idx e in enumeratearray sortedstartarray sortedpairarray keylambda e e0 beststart sortedstartarray01 bestleft sortedstartarray for e in sortedstartarray1 if e0 beststart if e1 maximprovement maximprovement improvement maxchoice i a j b printPolynomial solution print Swap d at d and d at d maxchoice1 maxchoice0 maxchoice3 maxchoice2 print Maximum beauty achieved is d score maximprovement text Single pass solution Swap 36 at 44 and 977 at 979 Maximum beauty achieved is 338168 Polynomial solution Swap 36 at 44 and 977 at 979 Maximum beauty achieved is 338168 An even faster way anevenfasterway Our strategy currently sort by start point the element with highest start point is the right side of swap among the elements with lower not lowest start points find the one with the lowest end point Notice that the second step is direct confirmation that right side of the swap has to be the element with the highest start point But how come we dont have a simple way to describe the left side of our swap Lets try that In step 3 we are finding an element with the lowest end point But this is among elements that already have low start points But wait if an element has the lowest end point then it doesnt mean it has the lowest start point imagine 0 10 5 6 and compare them So we have stumbled on a new finding For the left side of the swap we only care about the end point This makes sense because we care about the gaps start which is defined by the elements end point regardless of where it starts In our current strategy sorting helps in two ways 1 finding the right side of swap directly 2 while looking for the smallest end point we are able to cut down search space because its a sorted array again by exploiting transitivity A step back astepback Each element has two quantities a start point and an end point Our goal is to find two elements One with the highest start point One with the lowest end point and also a low start point Notice that we no longer care about both quantities in each element We either care about highest start point or about the lowest end point This is crucial Imagine we had an array of dictionaries Each dictionary has two quantities alpha and beta We are asked to find two elements one with lowest alpha and one with highest beta In this case we could simply iterate over the array improving our search as each new element is encountered We are exactly in that scenario We can now avoid sorting and simply iterate over the array This is because we updated our observation for the left side of the swap We are no longer looking for an element that has both good start and good end We are only looking good end For the right side of the swap we were clear that we need to find the highest start point regardless of its end point That should have given us a hint to explore the left side of the swap we need the lowest end point regardless of its start point Tests tests Lets remove the sorting Instead we do a simple iteration where we update our best start point and our best end point The end of the iteration signals the end of our search no transitivity as this is an unsorted array python array randomchoicelistrange0 1000 for i in range0 1000 pairarray mine idx maxe idx for idx e in enumeratearray beststart 1 bestleft pairarray0 bestend 10000 bestright pairarray1 for e in pairarray if e1 beststart beststart e0 bestright e score sumabse idx for idx e in enumeratearray score 2 beststart bestend printSingle pass solution print Swap d at d and d at d bestleft0 bestleft1 bestright0 bestright1 print Maximum beauty achieved is d score CHECK 1 array51 array984 array984 array51 score sumabse idx for idx e in enumeratearray printscore CHECK 2 n2 test score sumabse idx for idx e in enumeratearray maximprovement 0 for i a in enumeratearray for j b in enumeratearray improvement absja absib absia absjb if improvement maximprovement maximprovement improvement maxchoice i a j b printPolynomial solution print Swap d at d and d at d maxchoice1 maxchoice0 maxchoice3 maxchoice2 print Maximum beauty achieved is d score maximprovement text Single pass solution Swap 2 at 13 and 979 at 993 Maximum beauty achieved is 330202 Polynomial solution Swap 2 at 13 and 993 at 979 Maximum beauty achieved is 330202 "}]